# Production Docker Compose - Optimized for production deployment
# Use: docker-compose -f docker/docker-compose.prod.yml up

# Docker Compose for Zerg AI Agent Platform - Production Mode

# Production-specific logging settings
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

services:
  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-zerg}
      POSTGRES_USER: ${POSTGRES_USER:-zerg}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-zerg} -d ${POSTGRES_DB:-zerg}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - zerg-network
    logging: *default-logging

  # Backend API Service
  backend:
    build:
      context: .
      dockerfile: docker/backend.dockerfile
      target: production
      args:
        BUILD_ENV: production
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      ENVIRONMENT: production
      MODELS_CONFIG_PATH: /config/models.json
      DATABASE_URL: postgresql://${POSTGRES_USER:-zerg}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-zerg}
      JWT_SECRET: ${JWT_SECRET}
      FERNET_SECRET: ${FERNET_SECRET}
      TRIGGER_SIGNING_SECRET: ${TRIGGER_SIGNING_SECRET}
      AUTH_DISABLED: ${AUTH_DISABLED:-0}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ALLOWED_CORS_ORIGINS: ${ALLOWED_CORS_ORIGINS}
      LLM_TOKEN_STREAM: ${LLM_TOKEN_STREAM:-true}
      DEV_ADMIN: ${DEV_ADMIN:-0}
    volumes:
      - backend_static:/app/static
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - zerg-network
    logging: *default-logging
    # Security: read-only root filesystem
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Jarvis Web UI (Chat interface)
  jarvis-web:
    build:
      context: .
      dockerfile: apps/jarvis/apps/web/Dockerfile
    restart: unless-stopped
    environment:
      DOCKER: "1"
      VITE_ZERG_API_URL: ${API_BASE_URL:-https://api.swarmlet.com}
      VITE_JARVIS_DEVICE_SECRET: ${JARVIS_DEVICE_SECRET}
      VITE_OPENAI_API_KEY: ${OPENAI_API_KEY}
      VITE_VOICE_CONTEXT: "personal"
      VITE_JARVIS_ENABLE_REALTIME_BRIDGE: "true"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - zerg-network
    logging: *default-logging

  # Jarvis Server (OpenAI Realtime bridge)
  jarvis-server:
    build:
      context: .
      dockerfile: apps/jarvis/apps/server/Dockerfile
    restart: unless-stopped
    environment:
      PORT: 8787
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      SKIP_MCP: "1"
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8787/session"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - zerg-network
    logging: *default-logging

  # Frontend Service - React UI (switched from Rust/WASM)
  frontend:
    build:
      context: ./apps/zerg/frontend-web
      dockerfile: Dockerfile
      target: production
    restart: unless-stopped
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:80/ | grep -q 'AI Agent Platform'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - zerg-network
    logging: *default-logging
    # Security: read-only root filesystem
    read_only: true
    tmpfs:
      - /var/cache/nginx:noexec,nosuid,size=50m,uid=1000,gid=1000
      - /var/log/nginx:noexec,nosuid,size=10m,uid=1000,gid=1000
      - /var/run:noexec,nosuid,size=10m,uid=1000,gid=1000
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'

  # Reverse Proxy - Routes paths to internal services
  # Coolify/Caddy handles SSL, this handles path-based routing
  reverse-proxy:
    image: nginx:alpine
    restart: unless-stopped
    depends_on:
      - frontend
      - backend
      - jarvis-web
      - jarvis-server
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/conf.d/default.conf:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - zerg-network
    logging: *default-logging

networks:
  zerg-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  postgres_data:
    driver: local
  backend_static:
    driver: local
