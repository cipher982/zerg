#!/usr/bin/env python3
"""
Debug script to test the full LangGraph workflow execution.
"""

import asyncio
import sys
import logging
from pathlib import Path

# Add backend to path
sys.path.append(str(Path(__file__).parent))

# Configure detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

async def debug_full_workflow():
    """Debug the full LangGraph workflow execution."""
    
    print("üêõ Debugging Full LangGraph Workflow")
    print("=" * 60)
    
    # Simulate canvas_data with single agent node
    canvas_data = {
        "nodes": [
            {
                "node_id": "node_0",
                "node_type": "AgentIdentity", 
                "agent_id": 3,
                "text": "New Agent 81",
                "message": "Hello, please execute this task",
                "x": 280.0,
                "y": 178.0,
                "width": 200.0,
                "height": 80.0,
                "color": "#2ecc71",
                "is_selected": False,
                "is_dragging": False,
                "parent_id": None
            }
        ],
        "edges": []
    }
    
    print(f"üìä Canvas data: {canvas_data}")
    
    try:
        # Import and create the workflow engine
        from zerg.services.langgraph_workflow_engine import LangGraphWorkflowEngine, WorkflowState
        
        engine = LangGraphWorkflowEngine()
        
        print("\nüîß Building LangGraph...")
        
        # Build the graph
        from langgraph.checkpoint.memory import MemorySaver
        checkpointer = MemorySaver()
        graph = engine._build_langgraph(canvas_data, execution_id=999, checkpointer=checkpointer)
        
        print("‚úÖ LangGraph built successfully")
        print(f"üìà Graph type: {type(graph)}")
        
        # Test graph execution
        print("\nüöÄ Testing graph execution...")
        
        initial_state = WorkflowState(
            execution_id=999,
            node_outputs={},
            completed_nodes=[],
            error=None
        )
        
        config = {"configurable": {"thread_id": "test_workflow_999"}}
        
        print(f"üìä Initial state: {initial_state}")
        print(f"‚öôÔ∏è  Config: {config}")
        
        # Test streaming execution
        print("\nüåä Testing streaming execution...")
        chunk_count = 0
        final_state = None
        
        try:
            async for chunk in graph.astream(initial_state, config):
                chunk_count += 1
                print(f"üì¶ Chunk #{chunk_count}: {chunk}")
                
                if chunk:
                    # LangGraph returns chunks with node_id as key, containing the state update
                    for node_id, state_update in chunk.items():
                        print(f"   üîç Node {node_id} update: {state_update}")
                        if isinstance(state_update, dict):
                            final_state = state_update
                            completed = len(state_update.get("completed_nodes", []))
                            print(f"   ‚úÖ Nodes completed so far: {completed}")
                            
                            # Show node outputs
                            node_outputs = state_update.get("node_outputs", {})
                            if node_outputs:
                                print(f"   üìä Node outputs keys: {list(node_outputs.keys())}")
                                for output_node_id, output in node_outputs.items():
                                    print(f"   üîç {output_node_id}: {output.get('type', 'unknown')} - {output.get('response', 'no response')[:50]}...")
                else:
                    print(f"   ‚ö†Ô∏è  Empty chunk #{chunk_count}")
            
            print(f"\nüèÅ Streaming completed with {chunk_count} chunks")
            
            if final_state:
                completed_nodes = final_state.get("completed_nodes", [])
                print(f"‚úÖ Final completed nodes: {completed_nodes}")
                print(f"üìä Final node outputs: {list(final_state.get('node_outputs', {}).keys())}")
                
                if len(completed_nodes) > 0:
                    print("üéâ SUCCESS: Nodes were executed!")
                    return True
                else:
                    print("‚ùå ISSUE: No nodes were executed")
                    return False
            else:
                print("‚ùå ISSUE: No final state received")
                return False
                
        except Exception as e:
            print(f"‚ùå Error during streaming execution: {e}")
            import traceback
            traceback.print_exc()
            return False
        
    except Exception as e:
        print(f"‚ùå Error in workflow setup: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(debug_full_workflow())
    if success:
        print("\n‚úÖ Full workflow debug completed successfully!")
    else:
        print("\n‚ùå Full workflow debug found issues!")